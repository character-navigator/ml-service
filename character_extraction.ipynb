{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from ebooklib import epub\n",
    "from util import ebook_parser\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AxelLokrantz\\anaconda3\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "book_names = [\"charles-dickens_a-christmas-carol\", \"pride_and_prejudice\", \"george-orwell_animal-farm\"]\n",
    "book_name = book_names[1]\n",
    "\n",
    "books_path = \"data/books/\"\n",
    "epub_file_extension = \".epub\"\n",
    "book = epub.read_epub(books_path + book_name + epub_file_extension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ePub to dataframe.\n",
    "\n",
    "def process_ebook(book):\n",
    "    \n",
    "    def remove_newline_sentence(sentences_df):\n",
    "        return sentences_df['sentence'].replace(to_replace=[r\"\\\\\", r\"\\n\"], value=' ', regex=True)\n",
    "\n",
    "    words, _, _ = ebook_parser.process_book(book)\n",
    "    df = pd.DataFrame(words)\n",
    "    sentences_df = df.groupby('sid')['w'].apply(lambda x: ' '.join(x)).reset_index(name='sentence')\n",
    "    sentences_df['sentence'] = remove_newline_sentence(sentences_df)\n",
    "    \n",
    "    return sentences_df\n",
    "\n",
    "# Returns True if 'character' is a character.\n",
    "\n",
    "def is_alphabetic(character):\n",
    "    return bool(re.match('^[A-Za-zÀ-ÖØ-öø-ÿ]+$', character))\n",
    "\n",
    "# Divide book into appropriate token sized chunks.\n",
    "\n",
    "def chunkify(tokenizer, sentences_df):\n",
    "    merged_sentences = ' '.join(sentences_df['sentence'])\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size = 512, chunk_overlap = 0)\n",
    "    ner_chunks = text_splitter.split_text(merged_sentences)\n",
    "    return ner_chunks\n",
    "\n",
    "# Extract character names.\n",
    "\n",
    "def extract_character_names(ner_chunks, tokenizer):\n",
    "    nlp = pipeline(\"ner\", model=bert_model, tokenizer=tokenizer)\n",
    "    final_names = []\n",
    "\n",
    "    for chunk_index, ner_chunk in enumerate(ner_chunks):\n",
    "        names = []\n",
    "        ner_results = nlp(ner_chunk)\n",
    "\n",
    "        for entity in ner_results:\n",
    "            if entity['entity'] in ['B-PER', 'I-PER']:\n",
    "                clean_name, start, end = extract_full_name(entity['word'], entity['start'], entity['end'], ner_chunk)\n",
    "\n",
    "                if len(names) > 0 and abs(names[-1]['index'] - entity['index']) == 1 and \"##\" not in entity['word']:\n",
    "                    names[-1]['index'] += 1\n",
    "                    names[-1]['end'] = end\n",
    "                    names[-1]['word'] += \" \" + clean_name\n",
    "                elif len(names) == 0 or names[-1]['end'] < entity['end']:\n",
    "                    names.append({'index': entity['index'], 'word': clean_name, 'start': start, 'end': end, 'score': entity['score'], 'chunk': chunk_index})\n",
    "\n",
    "        final_names.extend(names)\n",
    "\n",
    "    return final_names\n",
    "\n",
    "# Help function to extract the full name.\n",
    "\n",
    "def extract_full_name(word, start, end, ner_chunk):\n",
    "    clean_name = word.replace(\"##\", \"\")\n",
    "    start -= 1\n",
    "    while start >= 0 and ner_chunk[start] != \" \" and is_alphabetic(ner_chunk[start]):\n",
    "        clean_name = ner_chunk[start] + clean_name\n",
    "        start -= 1\n",
    "    start += 1\n",
    "\n",
    "    while end < len(ner_chunk) and ner_chunk[end] != \" \" and is_alphabetic(ner_chunk[end]):\n",
    "        clean_name += ner_chunk[end]\n",
    "        end += 1\n",
    "\n",
    "    return clean_name, start, end\n",
    "\n",
    "# Removes non-character entries in a list.\n",
    "\n",
    "def filter_names(names):\n",
    "    titles_to_remove, honorifics_without_punctuation_and_space = get_titles_to_remove()\n",
    "    print(honorifics_without_punctuation_and_space)\n",
    "\n",
    "    def remove_exact_honorifics(names, honorifics_without_punctuation):\n",
    "        honorifics_with_punctuation = [honorific + \".\" for honorific in honorifics_without_punctuation]\n",
    "        return [name for name in names if name not in honorifics_without_punctuation + honorifics_with_punctuation] \n",
    "    \n",
    "\n",
    "    def remove_titles(name):\n",
    "        for title in titles_to_remove:\n",
    "            name = name.replace(title, \"\").strip()\n",
    "        return name\n",
    "    \n",
    "    def filter_odd_characters(lst):\n",
    "        filtered_list = []\n",
    "        for s in lst:\n",
    "            filtered_string = ''.join(char for char in s if re.match('[A-Za-zÀ-ÖØ-öø-ÿ.]', char) or char == ' ')\n",
    "            if all(ch == ' ' for ch in filtered_string[1:][::2]):\n",
    "                filtered_string = filtered_string.replace(' ', '')\n",
    "            filtered_list.append(filtered_string)\n",
    "        return filtered_list\n",
    "    \n",
    "    def remove_non_alphabetic(names):\n",
    "        filtered_names = []\n",
    "        for name in names:\n",
    "            filtered_name = ''.join(char for char in name if char.isalpha() or char == '.' or char == ' ')\n",
    "            filtered_names.append(filtered_name)\n",
    "        return filtered_names\n",
    "    \n",
    "    def capitalize_first_letter(names):\n",
    "        return [name[0].upper() + name[1:] for name in names]\n",
    "    \n",
    "    filtered_names = map(remove_titles, names)\n",
    "    filtered_names = remove_non_alphabetic(filtered_names)\n",
    "    filtered_names = filter_odd_characters(filtered_names)\n",
    "    filtered_names = filter(lambda x: x != \"\", filtered_names)\n",
    "    filtered_names = remove_exact_honorifics(filtered_names, honorifics_without_punctuation_and_space)\n",
    "    filtered_names = capitalize_first_letter(filtered_names)\n",
    "\n",
    "    return list(filtered_names)\n",
    "\n",
    "def get_titles_to_remove():\n",
    "    base_path = \"data/filter-lists/\"\n",
    "    honorifics_path = base_path + \"salutations-titles-honorifics/\"\n",
    "    honorifics_without_punctuation_file = honorifics_path + \"list-of-salutations-titles-honorifics-without-punctuation.txt\"\n",
    "    honorifics_file = honorifics_path + \"list-of-salutations-titles-honorifics.txt\"\n",
    "    relationships_file = base_path + \"list-of-family-relationships.txt\"\n",
    "    real_entities_file = base_path + \"list-of-real-entities.txt\"\n",
    "\n",
    "    with open(honorifics_without_punctuation_file, \"r\") as file:\n",
    "        honorifics_without_punctuation = file.readlines()\n",
    "        honorifics_without_punctuation = [honorific.strip() for honorific in honorifics_without_punctuation]\n",
    "        honorifics_without_punctuation_and_space = [honorific for honorific in honorifics_without_punctuation]\n",
    "        honorifics_with_punctuation = list(map(lambda honorific: honorific + \". \", honorifics_without_punctuation))\n",
    "        honorifics_without_punctuation = [honorific + ' ' for honorific in honorifics_without_punctuation]\n",
    "\n",
    "    with open(honorifics_file, \"r\") as file:\n",
    "        honorifics = file.readlines()\n",
    "        honorifics = [honorific.strip() + ' ' for honorific in honorifics]\n",
    "\n",
    "    with open(relationships_file, \"r\") as file:\n",
    "        relationships = file.readlines()\n",
    "        relationships = [relationship.strip() + ' ' for relationship in relationships]\n",
    "    \n",
    "    with open(real_entities_file, \"r\") as file:\n",
    "        real_entities = file.readlines()\n",
    "        real_entities = [real_entity.strip() for real_entity in real_entities]\n",
    "\n",
    "    return honorifics_with_punctuation + honorifics_without_punctuation + honorifics + relationships + real_entities, honorifics_without_punctuation_and_space\n",
    "\n",
    "def append_name_frequency(filtered_entities):\n",
    "    unique_names = set(filtered_entities)\n",
    "    unique_name_frequencies = []\n",
    "    names_only_occurring_once = []\n",
    "    for unique_name in unique_names: \n",
    "        occurrences_of_name = filtered_entities.count(unique_name)\n",
    "        if occurrences_of_name > 1:\n",
    "            unique_name_frequencies.append({ 'name': unique_name, 'frequency': occurrences_of_name })\n",
    "        else:\n",
    "            names_only_occurring_once.append(unique_name)\n",
    "    unique_names_sorted_by_frequency = sorted(unique_name_frequencies, key=lambda unique_name_frequency: unique_name_frequency['frequency'], reverse=True)\n",
    "    return unique_names_sorted_by_frequency\n",
    "\n",
    "def filter_entities(filtered_names_and_freq):\n",
    "    \n",
    "    def remove_last_name_if_present_in_multiple_names(filtered_names_and_freq):\n",
    "        candidates_to_remove = []\n",
    "\n",
    "        for name in [filtered_name_and_freq['name'] for filtered_name_and_freq in filtered_names_and_freq]:\n",
    "            first_space_index = name.find(\" \")\n",
    "            if first_space_index != -1:\n",
    "                candidates_to_remove.append({\"first_name\": name[:first_space_index], \"last_name\": name[first_space_index + 1:]})\n",
    "        \n",
    "        to_remove = []\n",
    "        \n",
    "        for candidate in candidates_to_remove:\n",
    "            candidates_excluding_current_candidate = [character for character in candidates_to_remove if candidate != character]\n",
    "            answer = next((character for character in candidates_excluding_current_candidate if character['last_name'] == candidate['last_name']), None)\n",
    "            if answer is not None:\n",
    "                to_remove.append(answer['last_name'])\n",
    "        \n",
    "        return [name for name in filtered_names_and_freq if name['name'] not in to_remove]\n",
    "\n",
    "    def process_entities_with_same_first_name(filtered_names_and_freq):\n",
    "        \n",
    "        def exist_entities_with_duplicate_first_name(name):\n",
    "\n",
    "            first_name = name[:first_space_index]\n",
    "            filtered_names_without_self = [character for character in filtered_names_and_freq if name != character['name']]\n",
    "\n",
    "            filtered_names_without_self_and_last_names = [character['name'].split(\" \")[0] for character in filtered_names_without_self if \" \" in character['name']]\n",
    "            return any(first_name == character_first_name for character_first_name in filtered_names_without_self_and_last_names)\n",
    "        \n",
    "        for filtered_name_and_freq in filtered_names_and_freq:\n",
    "            first_space_index = filtered_name_and_freq['name'].find(\" \")\n",
    "            \n",
    "            if first_space_index != -1 and exist_entities_with_duplicate_first_name(filtered_name_and_freq['name']):\n",
    "                filtered_names_and_freq = [character for character in filtered_names_and_freq if character['name'] != filtered_name_and_freq['name'][:first_space_index]]\n",
    "                    \n",
    "        return filtered_names_and_freq\n",
    "    \n",
    "    def remove_author(book, filtered_names_and_freq):\n",
    "        author = book.get_metadata('DC', 'creator')\n",
    "        if author != None:\n",
    "            author = author[0][0]\n",
    "            return [filtered_name_and_freq for filtered_name_and_freq in filtered_names_and_freq if author != filtered_name_and_freq['name']]\n",
    "        return filtered_names_and_freq\n",
    "\n",
    "    filtered_names_and_freq = remove_last_name_if_present_in_multiple_names(filtered_names_and_freq)\n",
    "    filtered_names_and_freq = process_entities_with_same_first_name(filtered_names_and_freq)\n",
    "    filtered_names_and_freq = remove_author(book, filtered_names_and_freq)\n",
    "\n",
    "    return filtered_names_and_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AxelLokrantz\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences_df = process_ebook(book)\n",
    "sentences_df.to_csv('data/sentences_df.csv', index=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "ner_chunks = chunkify(tokenizer, sentences_df)\n",
    "entities = extract_character_names(ner_chunks, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', 'Mrs', 'Ms', 'Mx', 'Dr', 'Capt', 'Cpt', 'Cllr', 'Col', 'Cpl', 'Drs', 'Lt', 'Mme', 'Prof', 'Pvt', 'Sgt', 'Sqr', 'St']\n"
     ]
    }
   ],
   "source": [
    "names = [entity for entity in entities if entity['score'] >= 0.95 and len(entity['word']) > 1]\n",
    "names = [entity['word'] for entity in names]\n",
    "\n",
    "filtered_names_and_freq = filter_names(names)\n",
    "filtered_names_and_freq = append_name_frequency(filtered_names_and_freq)\n",
    "filtered_entities_and_freq = filter_entities(filtered_names_and_freq)\n",
    "\n",
    "final_names_df = pd.DataFrame(filtered_entities_and_freq)\n",
    "final_names_df.to_csv(\"data/final_names.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\\'1.0\\' encoding=\\'utf-8\\'?>\\n<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:epub=\"http://www.idpf.org/2007/ops\" epub:prefix=\"z3998: http://www.daisy.org/z3998/2012/vocab/structure/#\" lang=\"en\" xml:lang=\"en\">\\n  <head/>\\n  <body><section id=\"preface\" role=\"doc-preface\" epub:type=\"preface\">\\n\\t\\t\\t<h2 epub:type=\"title\">Preface</h2>\\n\\t\\t\\t<p>I have endeavoured in this Ghostly little book to raise the Ghost of an Idea which shall not put my readers out of humour with themselves, with each other, with the season, or with me. May it haunt their house pleasantly, and no one wish to lay it.</p>\\n\\t\\t\\t<p>Their faithful Friend and Servant,</p>\\n\\t\\t\\t<footer>\\n\\t\\t\\t\\t<p class=\"epub-type-z3998-signature first-child\" epub:type=\"z3998:signature\">\\n\\t\\t\\t\\t\\t<abbr epub:type=\"z3998:given-name\">C. D.</abbr>\\n\\t\\t\\t\\t</p>\\n\\t\\t\\t\\t<p>December, 1843.</p>\\n\\t\\t\\t</footer>\\n\\t\\t</section>\\n\\t</body>\\n</html>\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AxelLokrantz\\anaconda3\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "def get_preface(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    \n",
    "    # Iterate over all items in the book\n",
    "    for item in book.get_items():\n",
    "        # Check if the item is the preface\n",
    "        if 'preface' in item.get_name().lower():\n",
    "            return item.get_content()\n",
    "\n",
    "# Usage\n",
    "epub_path = 'data/books/charles-dickens_a-christmas-carol.epub'\n",
    "print(get_preface(epub_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
