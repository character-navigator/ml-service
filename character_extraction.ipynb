{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from ebooklib import epub\n",
    "from util import ebook_parser\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os  \n",
    "import shutil  \n",
    "import zipfile  \n",
    "import re  \n",
    "\n",
    "def get_navpoints(book):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts navigation points from an e-book's table of contents (TOC) in NCX format.\n",
    "\n",
    "    This function searches for specific keywords within the TOC and returns a list of navigation points\n",
    "    that match these keywords. Each navigation point includes the text label, target string, and \n",
    "    start anchor ID.\n",
    "\n",
    "    Args:\n",
    "        book (object): An object representing the e-book, which should have a method `get_item_with_id`\n",
    "                       to retrieve items by their ID. The TOC should be identifiable by the ID 'ncx'.\n",
    "\n",
    "    Returns:\n",
    "        list of tuple: A list of tuples, where each tuple contains:\n",
    "                       - text (str): The text label of the navigation point.\n",
    "                       - target_string (str): The target string derived from the 'src' attribute.\n",
    "                       - start_anchor_id (str): The start anchor ID derived from the 'src' attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    toc = book.get_item_with_id('ncx')\n",
    "    toc_content = toc.content.decode('utf-8')\n",
    "    root = ET.fromstring(toc_content)\n",
    "\n",
    "    search_words = [\"title\", \"copyright\", \"dedication\", \"epigraph\", \"table of contents\",\n",
    "                    \"contents\", \"foreword\", \"preface\", \"acknowledgements\", \"introduction\", \"prologue\",\n",
    "                    \"epilogue\", \"afterword\", \"footnote\", \"endnote\", \"glossary\", \"index\", \"appendix\",\n",
    "                    \"appendices\", \"illustration\", \"bibliography\", \"references\", \"blurb\", \"praise\",\n",
    "                    \"biography\", \"project gutenberg\"]\n",
    "\n",
    "    navPoints = []\n",
    "\n",
    "    for navPoint in root.find('{http://www.daisy.org/z3986/2005/ncx/}navMap').iter('{http://www.daisy.org/z3986/2005/ncx/}navPoint'):\n",
    "        text = navPoint.find('{http://www.daisy.org/z3986/2005/ncx/}navLabel').find('{http://www.daisy.org/z3986/2005/ncx/}text').text\n",
    "        src = navPoint.find('{http://www.daisy.org/z3986/2005/ncx/}content').attrib['src']\n",
    "\n",
    "        if any(word.lower() in text.lower() for word in search_words):\n",
    "            target_string = src.split('.html', 1)[0]\n",
    "            start_anchor_id = src.split('.html', 1)[1]\n",
    "            navPoints.append((text, target_string, start_anchor_id))\n",
    "    return navPoints\n",
    "\n",
    "def generate_processed_book(book_path, book_name, epub_file_extension, navPoints):\n",
    "\n",
    "    \"\"\"\n",
    "    Processes an e-book by removing specified sections based on navigation points and re-zips it into a new EPUB file.\n",
    "\n",
    "    This function extracts an EPUB file, removes specified sections identified by navigation points, and then\n",
    "    re-packages the contents into a new EPUB file.\n",
    "\n",
    "    Args:\n",
    "        book_path (str): The file path to the original EPUB book.\n",
    "        book_name (str): The name to use for the processed EPUB book.\n",
    "        epub_file_extension (str): The file extension for the EPUB book (e.g., '.epub').\n",
    "        navPoints (list of tuple): A list of navigation points where each tuple contains:\n",
    "                                   - text (str): The text label of the navigation point.\n",
    "                                   - target_string (str): The target string derived from the 'src' attribute.\n",
    "                                   - start_anchor_id (str): The start anchor ID derived from the 'src' attribute.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def get_next_anchor_id(current_id):  \n",
    "        prefix, num = re.match(r'(.*?)(\\d+)$', current_id).groups()  \n",
    "        next_num = str(int(num) + 1).zfill(len(num))\n",
    "        return f'{prefix}{next_num}'\n",
    "      \n",
    "    temp_dir = 'temp_epub_extracted'  \n",
    "\n",
    "    with zipfile.ZipFile(book_path, 'r') as zip_ref:  \n",
    "        zip_ref.extractall(temp_dir)  \n",
    "    \n",
    "\n",
    "    for text, target_string, start_anchor_id in navPoints:  \n",
    "        start_anchor_id = start_anchor_id[1:]\n",
    "        \n",
    "        for root, dirs, files in os.walk(temp_dir):  \n",
    "            for file in files:  \n",
    "                if target_string in file:  \n",
    "                    file_path = os.path.join(root, file)  \n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:  \n",
    "                        content = f.read()  \n",
    "\n",
    "                    if start_anchor_id == 'pg-footer-heading':\n",
    "                        pattern = fr'(<[^>]*id=\"{re.escape(start_anchor_id)}\".*?>).*?(</body>)'  \n",
    "                    else:\n",
    "                        end_anchor_id = get_next_anchor_id(start_anchor_id)\n",
    "                        pattern = fr'(<[^>]*id=\"{re.escape(start_anchor_id)}\".*?>).*?(<[^>]*id=\"{re.escape(end_anchor_id)}\")'  \n",
    "\n",
    "                    content = re.sub(pattern, r'\\2', content, flags=re.DOTALL)  \n",
    "    \n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:  \n",
    "                        f.write(content)  \n",
    "    \n",
    "    new_epub_path = 'data/books-pre-processed/'+ book_name + '-pre-processed' + epub_file_extension\n",
    "    with zipfile.ZipFile(new_epub_path, 'w', zipfile.ZIP_DEFLATED) as zip_ref:  \n",
    "        for root, dirs, files in os.walk(temp_dir):  \n",
    "            for file in files:  \n",
    "                file_path = os.path.join(root, file)  \n",
    "                arcname = os.path.relpath(file_path, temp_dir)  \n",
    "                zip_ref.write(file_path, arcname)  \n",
    "    \n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AxelLokrantz\\anaconda3\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "book_names = [\"the-odyssey\", \"war-and-peace\", \"animal-farm\", \"a-christmas-carol\"]\n",
    "book_index = 1\n",
    "book_name = book_names[book_index]\n",
    "\n",
    "books_path = \"data/books/\"\n",
    "epub_file_extension = \".epub\"\n",
    "book_path = books_path + book_name + epub_file_extension\n",
    "\n",
    "book = epub.read_epub(book_path)\n",
    "navPoints = get_navpoints(book)\n",
    "generate_processed_book(book_path, book_name, epub_file_extension, navPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ebook(book):\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes an e-book to extract and clean sentences.\n",
    "\n",
    "    This function parses an e-book to extract words, groups them into sentences, and removes newline\n",
    "    characters and backslashes from the sentences.\n",
    "\n",
    "    Args:\n",
    "        book (object): An object representing the e-book to be processed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the processed sentences with the following columns:\n",
    "                      - 'sid': The sentence ID.\n",
    "                      - 'sentence': The cleaned sentence text.\n",
    "    \"\"\"\n",
    "\n",
    "    def remove_newline_sentence(sentences_df):\n",
    "        return sentences_df['sentence'].replace(to_replace=[r\"\\\\\", r\"\\n\"], value=' ', regex=True)\n",
    "\n",
    "    words, _, _ = ebook_parser.process_book(book)\n",
    "    df = pd.DataFrame(words)\n",
    "    sentences_df = df.groupby('sid')['w'].apply(lambda x: ' '.join(x)).reset_index(name='sentence')\n",
    "    sentences_df['sentence'] = remove_newline_sentence(sentences_df)\n",
    "    \n",
    "    return sentences_df\n",
    "\n",
    "def is_alphabetic(character):\n",
    "    \n",
    "    \"\"\"\n",
    "    Checks if a character is alphabetic.\n",
    "\n",
    "    This function uses a regular expression to determine if a character is alphabetic, considering\n",
    "    both uppercase and lowercase ASCII letters, as well as some extended Latin characters.\n",
    "\n",
    "    Args:\n",
    "        character (str): The character to be checked.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the character is alphabetic, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    return bool(re.match('^[A-Za-zÀ-ÖØ-öø-ÿ]+$', character))\n",
    "\n",
    "def chunkify(tokenizer, sentences_df):\n",
    "\n",
    "    \"\"\"\n",
    "    Splits text into chunks for Named Entity Recognition (NER) processing.\n",
    "\n",
    "    This function combines sentences from a DataFrame into a single text, splits the text into chunks\n",
    "    suitable for NER processing using a tokenizer, and returns the chunks.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: An instance of a tokenizer compatible with Hugging Face Transformers.\n",
    "        sentences_df (pd.DataFrame): A DataFrame containing sentences to be chunked, with a column\n",
    "                                     named 'sentence' containing the sentence text.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the NER chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    merged_sentences = ' '.join(sentences_df['sentence'])\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size = 512, chunk_overlap = 0)\n",
    "    ner_chunks = text_splitter.split_text(merged_sentences)\n",
    "    return ner_chunks\n",
    "\n",
    "def extract_character_names(ner_chunks, tokenizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts character names from Named Entity Recognition (NER) chunks.\n",
    "\n",
    "    This function processes NER chunks using a pretrained NER pipeline and extracts character names\n",
    "    from the recognized entities, focusing on entities tagged as person names.\n",
    "\n",
    "    Args:\n",
    "        ner_chunks (list): A list of strings representing the NER chunks.\n",
    "        tokenizer: An instance of a tokenizer compatible with Hugging Face Transformers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains information about a character name:\n",
    "              - 'index': The index of the character name in the original text.\n",
    "              - 'word': The extracted character name.\n",
    "              - 'start': The starting position of the character name in the original text.\n",
    "              - 'end': The ending position of the character name in the original text.\n",
    "              - 'score': The confidence score assigned to the character name extraction.\n",
    "              - 'chunk': The index of the NER chunk where the character name was found.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = pipeline(\"ner\", model=bert_model, tokenizer=tokenizer)\n",
    "    final_names = []\n",
    "\n",
    "    for chunk_index, ner_chunk in enumerate(ner_chunks):\n",
    "        names = []\n",
    "        ner_results = nlp(ner_chunk)\n",
    "\n",
    "        for entity in ner_results:\n",
    "            if entity['entity'] in ['B-PER', 'I-PER']:\n",
    "                clean_name, start, end = extract_full_name(entity['word'], entity['start'], entity['end'], ner_chunk)\n",
    "\n",
    "                if len(names) > 0 and abs(names[-1]['index'] - entity['index']) == 1 and \"##\" not in entity['word']:\n",
    "                    names[-1]['index'] += 1\n",
    "                    names[-1]['end'] = end\n",
    "                    names[-1]['word'] += \" \" + clean_name\n",
    "                elif len(names) == 0 or names[-1]['end'] < entity['end']:\n",
    "                    names.append({'index': entity['index'], 'word': clean_name, 'start': start, 'end': end, 'score': entity['score'], 'chunk': chunk_index})\n",
    "\n",
    "        final_names.extend(names)\n",
    "\n",
    "    return final_names\n",
    "\n",
    "def extract_full_name(word, start, end, ner_chunk):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts a full name from a word token within a Named Entity Recognition (NER) chunk.\n",
    "\n",
    "    This function retrieves the full name associated with a word token by expanding it to include\n",
    "    adjacent alphabetic characters, ensuring the entire name is captured.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word token representing part of a potential full name.\n",
    "        start (int): The starting index of the word token in the NER chunk.\n",
    "        end (int): The ending index of the word token in the NER chunk.\n",
    "        ner_chunk (str): The NER chunk containing the word token.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - The cleaned full name (str).\n",
    "               - The adjusted starting index of the full name in the NER chunk (int).\n",
    "               - The adjusted ending index of the full name in the NER chunk (int).\n",
    "    \"\"\"\n",
    "\n",
    "    clean_name = word.replace(\"##\", \"\")\n",
    "    start -= 1\n",
    "    while start >= 0 and ner_chunk[start] != \" \" and is_alphabetic(ner_chunk[start]):\n",
    "        clean_name = ner_chunk[start] + clean_name\n",
    "        start -= 1\n",
    "    start += 1\n",
    "\n",
    "    while end < len(ner_chunk) and ner_chunk[end] != \" \" and is_alphabetic(ner_chunk[end]):\n",
    "        clean_name += ner_chunk[end]\n",
    "        end += 1\n",
    "\n",
    "    return clean_name, start, end\n",
    "\n",
    "def filter_names(names):\n",
    "\n",
    "    \"\"\"\n",
    "    Filters and cleans a list of names.\n",
    "\n",
    "    This function processes a list of names by removing titles, odd characters, non-alphabetic characters,\n",
    "    empty strings, and exact honorifics, and capitalizing the first letter of each name.\n",
    "\n",
    "    Args:\n",
    "        names (list): A list of strings representing names.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of filtered and cleaned names.\n",
    "    \"\"\"\n",
    "\n",
    "    titles_to_remove, honorifics_without_punctuation_and_space = get_titles_to_remove()\n",
    "\n",
    "    def remove_exact_honorifics(names, honorifics_without_punctuation):\n",
    "        honorifics_with_punctuation = [honorific + \".\" for honorific in honorifics_without_punctuation]\n",
    "        return [name for name in names if name not in honorifics_without_punctuation + honorifics_with_punctuation] \n",
    "    \n",
    "\n",
    "    def remove_titles(name):\n",
    "        for title in titles_to_remove:\n",
    "            name = name.replace(title, \"\").strip()\n",
    "        return name\n",
    "    \n",
    "    def filter_odd_characters(lst):\n",
    "        filtered_list = []\n",
    "        for s in lst:\n",
    "            filtered_string = ''.join(char for char in s if re.match('[A-Za-zÀ-ÖØ-öø-ÿ.]', char) or char == ' ')\n",
    "            if all(ch == ' ' for ch in filtered_string[1:][::2]):\n",
    "                filtered_string = filtered_string.replace(' ', '')\n",
    "            filtered_list.append(filtered_string)\n",
    "        return filtered_list\n",
    "    \n",
    "    def remove_non_alphabetic(names):\n",
    "        filtered_names = []\n",
    "        for name in names:\n",
    "            filtered_name = ''.join(char for char in name if char.isalpha() or char == '.' or char == ' ')\n",
    "            filtered_names.append(filtered_name)\n",
    "        return filtered_names\n",
    "    \n",
    "    def capitalize_first_letter(names):\n",
    "        return [name[0].upper() + name[1:] for name in names]\n",
    "    \n",
    "    filtered_names = map(remove_titles, names)\n",
    "    filtered_names = remove_non_alphabetic(filtered_names)\n",
    "    filtered_names = filter_odd_characters(filtered_names)\n",
    "    filtered_names = filter(lambda x: x != \"\", filtered_names)\n",
    "    filtered_names = remove_exact_honorifics(filtered_names, honorifics_without_punctuation_and_space)\n",
    "    filtered_names = capitalize_first_letter(filtered_names)\n",
    "\n",
    "    return list(filtered_names)\n",
    "\n",
    "def get_titles_to_remove():\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieves lists of titles, honorifics, relationships, and real entities to be removed.\n",
    "\n",
    "    This function reads lists of titles, honorifics, relationships, and real entities from files,\n",
    "    preprocesses them, and returns two lists:\n",
    "    - One list contains titles, honorifics, relationships, and real entities with punctuation.\n",
    "    - The other list contains honorifics without punctuation or spaces.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "               - The first list includes titles, honorifics, relationships, and real entities with punctuation.\n",
    "               - The second list includes honorifics without punctuation or spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    base_path = \"data/filter-lists/\"\n",
    "    honorifics_path = base_path + \"salutations-titles-honorifics/\"\n",
    "    honorifics_without_punctuation_file = honorifics_path + \"list-of-salutations-titles-honorifics-without-punctuation.txt\"\n",
    "    honorifics_file = honorifics_path + \"list-of-salutations-titles-honorifics.txt\"\n",
    "    relationships_file = base_path + \"list-of-family-relationships.txt\"\n",
    "    real_entities_file = base_path + \"list-of-real-entities.txt\"\n",
    "\n",
    "    with open(honorifics_without_punctuation_file, \"r\") as file:\n",
    "        honorifics_without_punctuation = file.readlines()\n",
    "        honorifics_without_punctuation = [honorific.strip() for honorific in honorifics_without_punctuation]\n",
    "        honorifics_without_punctuation_and_space = [honorific for honorific in honorifics_without_punctuation]\n",
    "        honorifics_with_punctuation = list(map(lambda honorific: honorific + \". \", honorifics_without_punctuation))\n",
    "        honorifics_without_punctuation = [honorific + ' ' for honorific in honorifics_without_punctuation]\n",
    "\n",
    "    with open(honorifics_file, \"r\") as file:\n",
    "        honorifics = file.readlines()\n",
    "        honorifics = [honorific.strip() + ' ' for honorific in honorifics]\n",
    "\n",
    "    with open(relationships_file, \"r\") as file:\n",
    "        relationships = file.readlines()\n",
    "        relationships = [relationship.strip() + ' ' for relationship in relationships]\n",
    "    \n",
    "    with open(real_entities_file, \"r\") as file:\n",
    "        real_entities = file.readlines()\n",
    "        real_entities = [real_entity.strip() for real_entity in real_entities]\n",
    "\n",
    "    return honorifics_with_punctuation + honorifics_without_punctuation + honorifics + relationships + real_entities, honorifics_without_punctuation_and_space\n",
    "\n",
    "def append_name_frequency(filtered_entities):\n",
    "\n",
    "    \"\"\"\n",
    "    Appends frequency information to a list of filtered entities.\n",
    "\n",
    "    This function calculates the frequency of each unique name in a list of filtered entities\n",
    "    and appends this information to the list. Names occurring only once are excluded from the\n",
    "    frequency information.\n",
    "\n",
    "    Args:\n",
    "        filtered_entities (list): A list of strings representing filtered entities.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "              - 'name': The unique name.\n",
    "              - 'frequency': The frequency of occurrences of the name in the list.\n",
    "              The list is sorted by frequency in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    unique_names = set(filtered_entities)\n",
    "    unique_name_frequencies = []\n",
    "    names_only_occurring_once = []\n",
    "    for unique_name in unique_names: \n",
    "        occurrences_of_name = filtered_entities.count(unique_name)\n",
    "        if occurrences_of_name > 1:\n",
    "            unique_name_frequencies.append({ 'name': unique_name, 'frequency': occurrences_of_name })\n",
    "        else:\n",
    "            names_only_occurring_once.append(unique_name)\n",
    "    unique_names_sorted_by_frequency = sorted(unique_name_frequencies, key=lambda unique_name_frequency: unique_name_frequency['frequency'], reverse=True)\n",
    "    return unique_names_sorted_by_frequency\n",
    "\n",
    "def filter_entities(filtered_names_and_freq):\n",
    "\n",
    "    \"\"\"\n",
    "    Filters a list of entities based on various criteria.\n",
    "\n",
    "    This function filters a list of entities based on criteria such as removing last names if present\n",
    "    in multiple names, processing entities with the same first name, removing the book's author from\n",
    "    the list of entities, and removing plural last names.\n",
    "\n",
    "    Args:\n",
    "        filtered_names_and_freq (list): A list of dictionaries containing filtered names and their frequencies.\n",
    "\n",
    "    Returns:\n",
    "        list: A filtered list of dictionaries containing names and their frequencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def remove_last_name_if_present_in_multiple_names(filtered_names_and_freq):\n",
    "        candidates_to_remove = []\n",
    "\n",
    "        for name in [filtered_name_and_freq['name'] for filtered_name_and_freq in filtered_names_and_freq]:\n",
    "            first_space_index = name.find(\" \")\n",
    "            if first_space_index != -1:\n",
    "                candidates_to_remove.append({\"first_name\": name[:first_space_index], \"last_name\": name[first_space_index + 1:]})\n",
    "        \n",
    "        to_remove = []\n",
    "        \n",
    "        for candidate in candidates_to_remove:\n",
    "            candidates_excluding_current_candidate = [character for character in candidates_to_remove if candidate != character]\n",
    "            answer = next((character for character in candidates_excluding_current_candidate if character['last_name'] == candidate['last_name']), None)\n",
    "            if answer is not None:\n",
    "                to_remove.append(answer['last_name'])\n",
    "        \n",
    "        return [name for name in filtered_names_and_freq if name['name'] not in to_remove]\n",
    "\n",
    "    def process_entities_with_same_first_name(filtered_names_and_freq):\n",
    "        \n",
    "        def exist_entities_with_duplicate_first_name(name):\n",
    "\n",
    "            first_name = name[:first_space_index]\n",
    "            filtered_names_without_self = [character for character in filtered_names_and_freq if name != character['name']]\n",
    "\n",
    "            filtered_names_without_self_and_last_names = [character['name'].split(\" \")[0] for character in filtered_names_without_self if \" \" in character['name']]\n",
    "            return any(first_name == character_first_name for character_first_name in filtered_names_without_self_and_last_names)\n",
    "        \n",
    "        for filtered_name_and_freq in filtered_names_and_freq:\n",
    "            first_space_index = filtered_name_and_freq['name'].find(\" \")\n",
    "            \n",
    "            if first_space_index != -1 and exist_entities_with_duplicate_first_name(filtered_name_and_freq['name']):\n",
    "                filtered_names_and_freq = [character for character in filtered_names_and_freq if character['name'] != filtered_name_and_freq['name'][:first_space_index]]\n",
    "                    \n",
    "        return filtered_names_and_freq\n",
    "    \n",
    "    def remove_author(book, filtered_names_and_freq):\n",
    "        author = book.get_metadata('DC', 'creator')\n",
    "        if author != None:\n",
    "            author = author[0][0]\n",
    "            author_split = author.split()\n",
    "            if len(author_split) > 2:\n",
    "                author = author_split[1] + \" \" + author_split[2]\n",
    "            return [filtered_name_and_freq for filtered_name_and_freq in filtered_names_and_freq if author != filtered_name_and_freq['name']]\n",
    "        return filtered_names_and_freq\n",
    "    \n",
    "    def remove_plural_lastnames(filtered_names_and_freq):\n",
    "        lastnames = [name['name'].split(' ')[-1] for name in filtered_names_and_freq if ' ' in name['name']]\n",
    "        filtered_names_and_freq = [name for name in filtered_names_and_freq\n",
    "            if not (name['name'].endswith('s') and name['name'][:-1] in lastnames)]\n",
    "        return filtered_names_and_freq\n",
    "\n",
    "    filtered_names_and_freq = remove_last_name_if_present_in_multiple_names(filtered_names_and_freq)\n",
    "    filtered_names_and_freq = process_entities_with_same_first_name(filtered_names_and_freq)\n",
    "    filtered_names_and_freq = remove_author(book, filtered_names_and_freq)\n",
    "    filtered_names_and_freq = remove_plural_lastnames(filtered_names_and_freq)\n",
    "\n",
    "    return filtered_names_and_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AxelLokrantz\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "book_path = 'data/books-pre-processed/' + book_name + '-pre-processed' + epub_file_extension\n",
    "book = epub.read_epub(book_path)\n",
    "\n",
    "sentences_df = process_ebook(book)\n",
    "sentences_df.to_csv('data/sentences/' + book_names[book_index] + '_sentences.csv', index=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "ner_chunks = chunkify(tokenizer, sentences_df)\n",
    "entities = extract_character_names(ner_chunks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [entity for entity in entities if entity['score'] >= 0.95 and len(entity['word']) > 1]\n",
    "names = [entity['word'] for entity in names]\n",
    "\n",
    "filtered_names_and_freq = filter_names(names)\n",
    "filtered_names_and_freq = append_name_frequency(filtered_names_and_freq)\n",
    "filtered_entities_and_freq = filter_entities(filtered_names_and_freq)\n",
    "\n",
    "final_names_df = pd.DataFrame(filtered_entities_and_freq)\n",
    "final_names_df.to_csv(f\"data/characters/{book_name}_characters.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
