{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from ebooklib import epub\n",
    "from util import ebook_parser\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6578f83fe148dbbdff08dda103ebe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DanielDahlberg\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DanielDahlberg\\.cache\\huggingface\\hub\\models--dslim--bert-large-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8c7b4bee0f48f4bd2215b5adf8cdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os  \n",
    "import shutil  \n",
    "import zipfile  \n",
    "import re  \n",
    "\n",
    "def get_navpoints(book):\n",
    "    toc = book.get_item_with_id('ncx')\n",
    "    toc_content = toc.content.decode('utf-8')\n",
    "    root = ET.fromstring(toc_content)\n",
    "\n",
    "    search_words = [\"title\", \"copyright\", \"dedication\", \"epigraph\", \"table of contents\",\n",
    "                    \"contents\", \"foreword\", \"preface\", \"acknowledgements\", \"introduction\", \"prologue\",\n",
    "                    \"epilogue\", \"afterword\", \"footnote\", \"endnote\", \"glossary\", \"index\", \"appendix\",\n",
    "                    \"appendices\", \"illustration\", \"bibliography\", \"references\", \"blurb\", \"praise\",\n",
    "                    \"biography\", \"project gutenberg\"]\n",
    "\n",
    "    navPoints = []\n",
    "\n",
    "    for navPoint in root.find('{http://www.daisy.org/z3986/2005/ncx/}navMap').iter('{http://www.daisy.org/z3986/2005/ncx/}navPoint'):\n",
    "        text = navPoint.find('{http://www.daisy.org/z3986/2005/ncx/}navLabel').find('{http://www.daisy.org/z3986/2005/ncx/}text').text\n",
    "        src = navPoint.find('{http://www.daisy.org/z3986/2005/ncx/}content').attrib['src']\n",
    "\n",
    "        if any(word.lower() in text.lower() for word in search_words):\n",
    "            target_string = src.split('.html', 1)[0]\n",
    "            start_anchor_id = src.split('.html', 1)[1]\n",
    "            navPoints.append((text, target_string, start_anchor_id))\n",
    "    return navPoints\n",
    "\n",
    "def generate_processed_book(book_path, book_name, epub_file_extension, navPoints):\n",
    "    def get_next_anchor_id(current_id):  \n",
    "        prefix, num = re.match(r'(.*?)(\\d+)$', current_id).groups()  \n",
    "        next_num = str(int(num) + 1).zfill(len(num))\n",
    "        return f'{prefix}{next_num}'\n",
    "      \n",
    "    temp_dir = 'temp_epub_extracted'  \n",
    "\n",
    "    with zipfile.ZipFile(book_path, 'r') as zip_ref:  \n",
    "        zip_ref.extractall(temp_dir)  \n",
    "    \n",
    "\n",
    "    for text, target_string, start_anchor_id in navPoints:  \n",
    "        start_anchor_id = start_anchor_id[1:]  # Remove the '#' from the start_anchor_id  \n",
    "        \n",
    "        for root, dirs, files in os.walk(temp_dir):  \n",
    "            for file in files:  \n",
    "                if target_string in file:  \n",
    "                    file_path = os.path.join(root, file)  \n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:  \n",
    "                        content = f.read()  \n",
    "\n",
    "                    if start_anchor_id == 'pg-footer-heading':\n",
    "                        pattern = fr'(<[^>]*id=\"{re.escape(start_anchor_id)}\".*?>).*?(</body>)'  \n",
    "                    else:\n",
    "                        end_anchor_id = get_next_anchor_id(start_anchor_id)  # Assuming the ID starts with '#'\n",
    "                        # print(end_anchor_id)\n",
    "                        # print(start_anchor_id) \n",
    "                        pattern = fr'(<[^>]*id=\"{re.escape(start_anchor_id)}\".*?>).*?(<[^>]*id=\"{re.escape(end_anchor_id)}\")'  \n",
    "\n",
    "                    content = re.sub(pattern, r'\\2', content, flags=re.DOTALL)  \n",
    "    \n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:  \n",
    "                        f.write(content)  \n",
    "                    # print(f\"Modified file: {file}\") \n",
    "    \n",
    "    # Zip the contents back into a new EPUB file  \n",
    "    new_epub_path = 'data/books-pre-processed/'+ book_name + '-pre-processed' + epub_file_extension\n",
    "    with zipfile.ZipFile(new_epub_path, 'w', zipfile.ZIP_DEFLATED) as zip_ref:  \n",
    "        for root, dirs, files in os.walk(temp_dir):  \n",
    "            for file in files:  \n",
    "                file_path = os.path.join(root, file)  \n",
    "                arcname = os.path.relpath(file_path, temp_dir)  \n",
    "                zip_ref.write(file_path, arcname)  \n",
    "    \n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DanielDahlberg\\anaconda3\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n"
     ]
    }
   ],
   "source": [
    "book_names = [\"the-odyssey\", \"war-and-peace\", \"animal-farm\", \"a-christmas-carol\"]\n",
    "book_index = 2\n",
    "book_name = book_names[book_index]\n",
    "\n",
    "books_path = \"data/books/\"\n",
    "epub_file_extension = \".epub\"\n",
    "book_path = books_path + book_name + epub_file_extension\n",
    "\n",
    "book = epub.read_epub(book_path)\n",
    "navPoints = get_navpoints(book)\n",
    "generate_processed_book(book_path, book_name, epub_file_extension, navPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ePub to dataframe.\n",
    "\n",
    "def process_ebook(book):\n",
    "    \n",
    "    def remove_newline_sentence(sentences_df):\n",
    "        return sentences_df['sentence'].replace(to_replace=[r\"\\\\\", r\"\\n\"], value=' ', regex=True)\n",
    "\n",
    "    words, _, _ = ebook_parser.process_book(book)\n",
    "    df = pd.DataFrame(words)\n",
    "    sentences_df = df.groupby('sid')['w'].apply(lambda x: ' '.join(x)).reset_index(name='sentence')\n",
    "    sentences_df['sentence'] = remove_newline_sentence(sentences_df)\n",
    "    \n",
    "    return sentences_df\n",
    "\n",
    "# Returns True if 'character' is a character.\n",
    "\n",
    "def is_alphabetic(character):\n",
    "    return bool(re.match('^[A-Za-zÀ-ÖØ-öø-ÿ]+$', character))\n",
    "\n",
    "# Divide book into appropriate token sized chunks.\n",
    "\n",
    "def chunkify(tokenizer, sentences_df):\n",
    "    merged_sentences = ' '.join(sentences_df['sentence'])\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size = 512, chunk_overlap = 0)\n",
    "    ner_chunks = text_splitter.split_text(merged_sentences)\n",
    "    return ner_chunks\n",
    "\n",
    "# Extract character names.\n",
    "\n",
    "def extract_character_names(ner_chunks, tokenizer):\n",
    "    nlp = pipeline(\"ner\", model=bert_model, tokenizer=tokenizer)\n",
    "    final_names = []\n",
    "\n",
    "    for chunk_index, ner_chunk in enumerate(ner_chunks):\n",
    "        names = []\n",
    "        ner_results = nlp(ner_chunk)\n",
    "\n",
    "        for entity in ner_results:\n",
    "            if entity['entity'] in ['B-PER', 'I-PER']:\n",
    "                clean_name, start, end = extract_full_name(entity['word'], entity['start'], entity['end'], ner_chunk)\n",
    "\n",
    "                if len(names) > 0 and abs(names[-1]['index'] - entity['index']) == 1 and \"##\" not in entity['word']:\n",
    "                    names[-1]['index'] += 1\n",
    "                    names[-1]['end'] = end\n",
    "                    names[-1]['word'] += \" \" + clean_name\n",
    "                elif len(names) == 0 or names[-1]['end'] < entity['end']:\n",
    "                    names.append({'index': entity['index'], 'word': clean_name, 'start': start, 'end': end, 'score': entity['score'], 'chunk': chunk_index})\n",
    "\n",
    "        final_names.extend(names)\n",
    "\n",
    "    return final_names\n",
    "\n",
    "# Help function to extract the full name.\n",
    "\n",
    "def extract_full_name(word, start, end, ner_chunk):\n",
    "    clean_name = word.replace(\"##\", \"\")\n",
    "    start -= 1\n",
    "    while start >= 0 and ner_chunk[start] != \" \" and is_alphabetic(ner_chunk[start]):\n",
    "        clean_name = ner_chunk[start] + clean_name\n",
    "        start -= 1\n",
    "    start += 1\n",
    "\n",
    "    while end < len(ner_chunk) and ner_chunk[end] != \" \" and is_alphabetic(ner_chunk[end]):\n",
    "        clean_name += ner_chunk[end]\n",
    "        end += 1\n",
    "\n",
    "    return clean_name, start, end\n",
    "\n",
    "# Removes non-character entries in a list.\n",
    "\n",
    "def filter_names(names):\n",
    "    titles_to_remove, honorifics_without_punctuation_and_space = get_titles_to_remove()\n",
    "\n",
    "    def remove_exact_honorifics(names, honorifics_without_punctuation):\n",
    "        honorifics_with_punctuation = [honorific + \".\" for honorific in honorifics_without_punctuation]\n",
    "        return [name for name in names if name not in honorifics_without_punctuation + honorifics_with_punctuation] \n",
    "    \n",
    "\n",
    "    def remove_titles(name):\n",
    "        for title in titles_to_remove:\n",
    "            name = name.replace(title, \"\").strip()\n",
    "        return name\n",
    "    \n",
    "    def filter_odd_characters(lst):\n",
    "        filtered_list = []\n",
    "        for s in lst:\n",
    "            filtered_string = ''.join(char for char in s if re.match('[A-Za-zÀ-ÖØ-öø-ÿ.]', char) or char == ' ')\n",
    "            if all(ch == ' ' for ch in filtered_string[1:][::2]):\n",
    "                filtered_string = filtered_string.replace(' ', '')\n",
    "            filtered_list.append(filtered_string)\n",
    "        return filtered_list\n",
    "    \n",
    "    def remove_non_alphabetic(names):\n",
    "        filtered_names = []\n",
    "        for name in names:\n",
    "            filtered_name = ''.join(char for char in name if char.isalpha() or char == '.' or char == ' ')\n",
    "            filtered_names.append(filtered_name)\n",
    "        return filtered_names\n",
    "    \n",
    "    def capitalize_first_letter(names):\n",
    "        return [name[0].upper() + name[1:] for name in names]\n",
    "    \n",
    "    filtered_names = map(remove_titles, names)\n",
    "    filtered_names = remove_non_alphabetic(filtered_names)\n",
    "    filtered_names = filter_odd_characters(filtered_names)\n",
    "    filtered_names = filter(lambda x: x != \"\", filtered_names)\n",
    "    filtered_names = remove_exact_honorifics(filtered_names, honorifics_without_punctuation_and_space)\n",
    "    filtered_names = capitalize_first_letter(filtered_names)\n",
    "\n",
    "    return list(filtered_names)\n",
    "\n",
    "def get_titles_to_remove():\n",
    "    base_path = \"data/filter-lists/\"\n",
    "    honorifics_path = base_path + \"salutations-titles-honorifics/\"\n",
    "    honorifics_without_punctuation_file = honorifics_path + \"list-of-salutations-titles-honorifics-without-punctuation.txt\"\n",
    "    honorifics_file = honorifics_path + \"list-of-salutations-titles-honorifics.txt\"\n",
    "    relationships_file = base_path + \"list-of-family-relationships.txt\"\n",
    "    real_entities_file = base_path + \"list-of-real-entities.txt\"\n",
    "\n",
    "    with open(honorifics_without_punctuation_file, \"r\") as file:\n",
    "        honorifics_without_punctuation = file.readlines()\n",
    "        honorifics_without_punctuation = [honorific.strip() for honorific in honorifics_without_punctuation]\n",
    "        honorifics_without_punctuation_and_space = [honorific for honorific in honorifics_without_punctuation]\n",
    "        honorifics_with_punctuation = list(map(lambda honorific: honorific + \". \", honorifics_without_punctuation))\n",
    "        honorifics_without_punctuation = [honorific + ' ' for honorific in honorifics_without_punctuation]\n",
    "\n",
    "    with open(honorifics_file, \"r\") as file:\n",
    "        honorifics = file.readlines()\n",
    "        honorifics = [honorific.strip() + ' ' for honorific in honorifics]\n",
    "\n",
    "    with open(relationships_file, \"r\") as file:\n",
    "        relationships = file.readlines()\n",
    "        relationships = [relationship.strip() + ' ' for relationship in relationships]\n",
    "    \n",
    "    with open(real_entities_file, \"r\") as file:\n",
    "        real_entities = file.readlines()\n",
    "        real_entities = [real_entity.strip() for real_entity in real_entities]\n",
    "\n",
    "    return honorifics_with_punctuation + honorifics_without_punctuation + honorifics + relationships + real_entities, honorifics_without_punctuation_and_space\n",
    "\n",
    "def append_name_frequency(filtered_entities):\n",
    "    unique_names = set(filtered_entities)\n",
    "    unique_name_frequencies = []\n",
    "    names_only_occurring_once = []\n",
    "    for unique_name in unique_names: \n",
    "        occurrences_of_name = filtered_entities.count(unique_name)\n",
    "        if occurrences_of_name > 1:\n",
    "            unique_name_frequencies.append({ 'name': unique_name, 'frequency': occurrences_of_name })\n",
    "        else:\n",
    "            names_only_occurring_once.append(unique_name)\n",
    "    unique_names_sorted_by_frequency = sorted(unique_name_frequencies, key=lambda unique_name_frequency: unique_name_frequency['frequency'], reverse=True)\n",
    "    return unique_names_sorted_by_frequency\n",
    "\n",
    "def filter_entities(filtered_names_and_freq):\n",
    "    \n",
    "    def remove_last_name_if_present_in_multiple_names(filtered_names_and_freq):\n",
    "        candidates_to_remove = []\n",
    "\n",
    "        for name in [filtered_name_and_freq['name'] for filtered_name_and_freq in filtered_names_and_freq]:\n",
    "            first_space_index = name.find(\" \")\n",
    "            if first_space_index != -1:\n",
    "                candidates_to_remove.append({\"first_name\": name[:first_space_index], \"last_name\": name[first_space_index + 1:]})\n",
    "        \n",
    "        to_remove = []\n",
    "        \n",
    "        for candidate in candidates_to_remove:\n",
    "            candidates_excluding_current_candidate = [character for character in candidates_to_remove if candidate != character]\n",
    "            answer = next((character for character in candidates_excluding_current_candidate if character['last_name'] == candidate['last_name']), None)\n",
    "            if answer is not None:\n",
    "                to_remove.append(answer['last_name'])\n",
    "        \n",
    "        return [name for name in filtered_names_and_freq if name['name'] not in to_remove]\n",
    "\n",
    "    def process_entities_with_same_first_name(filtered_names_and_freq):\n",
    "        \n",
    "        def exist_entities_with_duplicate_first_name(name):\n",
    "\n",
    "            first_name = name[:first_space_index]\n",
    "            filtered_names_without_self = [character for character in filtered_names_and_freq if name != character['name']]\n",
    "\n",
    "            filtered_names_without_self_and_last_names = [character['name'].split(\" \")[0] for character in filtered_names_without_self if \" \" in character['name']]\n",
    "            return any(first_name == character_first_name for character_first_name in filtered_names_without_self_and_last_names)\n",
    "        \n",
    "        for filtered_name_and_freq in filtered_names_and_freq:\n",
    "            first_space_index = filtered_name_and_freq['name'].find(\" \")\n",
    "            \n",
    "            if first_space_index != -1 and exist_entities_with_duplicate_first_name(filtered_name_and_freq['name']):\n",
    "                filtered_names_and_freq = [character for character in filtered_names_and_freq if character['name'] != filtered_name_and_freq['name'][:first_space_index]]\n",
    "                    \n",
    "        return filtered_names_and_freq\n",
    "    \n",
    "    def remove_author(book, filtered_names_and_freq):\n",
    "        author = book.get_metadata('DC', 'creator')\n",
    "        if author != None:\n",
    "            author = author[0][0]\n",
    "            return [filtered_name_and_freq for filtered_name_and_freq in filtered_names_and_freq if author != filtered_name_and_freq['name']]\n",
    "        return filtered_names_and_freq\n",
    "    \n",
    "    def remove_plural_lastnames(filtered_names_and_freq):\n",
    "        lastnames = [name['name'].split(' ')[-1] for name in filtered_names_and_freq if ' ' in name['name']]\n",
    "        filtered_names_and_freq = [name for name in filtered_names_and_freq\n",
    "            if not (name['name'].endswith('s') and name['name'][:-1] in lastnames)]\n",
    "        return filtered_names_and_freq\n",
    "\n",
    "    filtered_names_and_freq = remove_last_name_if_present_in_multiple_names(filtered_names_and_freq)\n",
    "    filtered_names_and_freq = process_entities_with_same_first_name(filtered_names_and_freq)\n",
    "    filtered_names_and_freq = remove_author(book, filtered_names_and_freq)\n",
    "    filtered_names_and_freq = remove_plural_lastnames(filtered_names_and_freq)\n",
    "\n",
    "    return filtered_names_and_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DanielDahlberg\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5020bad0db542d1877d444c30266e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1b5504bd7648469a11351d29b98b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23eb05eb05c4f338cb875205ebbb311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "book_path = 'data/books-pre-processed/' + book_name + '-pre-processed' + epub_file_extension\n",
    "book = epub.read_epub(book_path)\n",
    "\n",
    "sentences_df = process_ebook(book)\n",
    "sentences_df.to_csv('data/sentences/' + book_names[book_index] + '_sentences.csv', index=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "ner_chunks = chunkify(tokenizer, sentences_df)\n",
    "entities = extract_character_names(ner_chunks, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [entity for entity in entities if entity['score'] >= 0.95 and len(entity['word']) > 1]\n",
    "names = [entity['word'] for entity in names]\n",
    "\n",
    "filtered_names_and_freq = filter_names(names)\n",
    "filtered_names_and_freq = append_name_frequency(filtered_names_and_freq)\n",
    "filtered_entities_and_freq = filter_entities(filtered_names_and_freq)\n",
    "\n",
    "final_names_df = pd.DataFrame(filtered_entities_and_freq)\n",
    "final_names_df.to_csv(f\"data/characters/{book_name}_characters.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
