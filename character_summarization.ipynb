{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_titles = [\"The Odyssey\", \"War and Peace\", \"Animal Farm\", \"Christmas Carol\"]\n",
    "book_names = [\"the-odyssey\", \"war-and-peace\", \"animal-farm\", \"a-christmas-carol\"]\n",
    "book_index = 2\n",
    "book_title = book_titles[book_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_message(role, content):\n",
    "    return {\"role\":role,\"content\":content}\n",
    "\n",
    "def generate_prompt(prompt):\n",
    "    return generate_message(role=\"user\", content=prompt)\n",
    "    \n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def generate_chunks(sentences_df, sentences_per_chunk, MAX_CHUNK_TOKEN_SIZE, ENCODING_NAME):\n",
    "    start = 0\n",
    "    chunks = []\n",
    "\n",
    "    def has_remaining_sentences():\n",
    "        return start < len(sentences_df)\n",
    "\n",
    "    def concatenate_sentences(df, start, end):\n",
    "        return df.loc[start:end, 'sentence'].str.cat()\n",
    "\n",
    "    while has_remaining_sentences():\n",
    "        end = start + sentences_per_chunk - 1\n",
    "        concatenated_sentences = concatenate_sentences(sentences_df, start, end)\n",
    "\n",
    "        # if end > len(sentences_df):\n",
    "        #     chunks.append(sentences_df.loc[start:len(sentences_df), 'sentence'].str.cat())\n",
    "        #     break\n",
    "\n",
    "        while num_tokens_from_string(concatenated_sentences, ENCODING_NAME) < MAX_CHUNK_TOKEN_SIZE and end < len(sentences_df) - 1:\n",
    "            end += 1\n",
    "            concatenated_sentences = concatenate_sentences(sentences_df, start, end)\n",
    "            \n",
    "\n",
    "        while num_tokens_from_string(concatenated_sentences, ENCODING_NAME) > MAX_CHUNK_TOKEN_SIZE:\n",
    "            end -= 1\n",
    "            concatenated_sentences = concatenate_sentences(sentences_df, start, end)\n",
    "\n",
    "        chunks.append({\"content\": concatenated_sentences, \"end_sid\": end})\n",
    "        start = end + 1\n",
    "    \n",
    "    # for chunk in chunks:\n",
    "    #     print(num_tokens_from_string(chunk, \"cl100k_base\"))\n",
    "    return chunks\n",
    "\n",
    "def calculate_sentences_per_chunk(sentences_df, ENCONDING_NAME, MAX_CHUNK_TOKEN_SIZE):\n",
    "\n",
    "    book = sentences_df['sentence'].astype(str).str.cat(sep=' ')\n",
    "\n",
    "    total_tokens = num_tokens_from_string(book, ENCONDING_NAME)\n",
    "    total_sentences = max(sentences_df['sid'])\n",
    "    avg_tokens_per_sentence = total_tokens / total_sentences\n",
    "    sentences_per_chunk = MAX_CHUNK_TOKEN_SIZE / avg_tokens_per_sentence\n",
    "\n",
    "    return sentences_per_chunk\n",
    "\n",
    "def parse_character_names(character_names):\n",
    "    names = character_names[0]\n",
    "    for index in range(1, len(character_names)):\n",
    "        if index < len(character_names) - 1:\n",
    "            names += \", \"\n",
    "        else:\n",
    "            names += \" and \"\n",
    "        names += character_names[index]\n",
    "    return names\n",
    "\n",
    "def parse_character_json_format(character_names):\n",
    "    names = f'\"{character_names[0]}\": Summary for {character_names[0]}'\n",
    "    for index in range(1, len(character_names)):\n",
    "        if index < len(character_names):\n",
    "            names += \",\\n\"\n",
    "        else:\n",
    "            names += \"\\n\"\n",
    "        names += f'\\t\"{character_names[index]}\": Summary for {character_names[index]}'\n",
    "    return names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "\n",
    "summarize_character_from_excerpt_template = PromptTemplate(\n",
    "    input_variables=[\"text\", \"parsed_character_names\", \"parsed_character_json_format\"],\n",
    "    template=\"\"\"I will provide you with a set of instructions, followed by some text.\n",
    "    The former is labeled 'Instructions' and the latter 'Text'.\n",
    "    \n",
    "    Instructions:\n",
    "    Briefly summarize the characters {parsed_character_names} based on Text.\n",
    "    For example, you can write about relations to other characters, personality traits, appearance, or other things you might find interesting.\n",
    "    Do not mention the source, i.e. that it is a text, book or summary anywhere.\n",
    "    Return the summaries as an JSON object according to the following format:\n",
    "    \n",
    "    {{\n",
    "        {parsed_character_json_format}\n",
    "    }}\n",
    "    \n",
    "    Text:\n",
    "    {text}\"\"\"\n",
    ")\n",
    "\n",
    "summarize_character_from_summaries_template = PromptTemplate(\n",
    "    input_variables=[\"character_name\", \"summaries\"],\n",
    "    template=\"\"\"I will provide you with a set of instructions, followed by a sequence of summaries.\n",
    "    The summaries are labeled 'Summary #1' and 'Summary #2'.\n",
    "\n",
    "    Instructions:\n",
    "    Briefly summarize the character {character_name} based on Summary #1 and Summary #2.\n",
    "\n",
    "    {summaries}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Messages\n",
    "\n",
    "system_message = generate_message(\n",
    "    role=\"system\", \n",
    "    content=\"You are a book publisher that summarizes a character based on an excerpt from a literary book.\")\n",
    "\n",
    "# user_message_example_1 = generate_message(\n",
    "#     role=\"user\",\n",
    "#     content=\"Briefly summarize the character (character name) based on the text below in one to two sentences. Focus on relations to other characters and personality traits.\\n\\n(text)\"\n",
    "# )\n",
    "\n",
    "user_message_example_1 = generate_message(\n",
    "    role=\"user\",\n",
    "    content=\"\"\"I will provide you with a set of instructions, followed by some text.\n",
    "    The former is labeled 'Instructions' and the latter 'Text'.\n",
    "    \n",
    "    Instructions:\n",
    "    Briefly summarize the characters (placeholder for character names) based on Text.\n",
    "    For example, you can write about relations to other characters, personality traits, appearance, or other things you might find interesting.\n",
    "    Do not mention the source, i.e. that it is a text, book or summary anywhere.\n",
    "    Return the summaries as an JSON object according to the following format:\n",
    "    \n",
    "    {{\n",
    "        (placeholder for properties of JSON object)\n",
    "    }}\n",
    "    \n",
    "    Text:\n",
    "    (some text)\"\"\"\n",
    ")\n",
    "\n",
    "assistant_message_example_1 = generate_message(\n",
    "    role=\"assistant\",\n",
    "    content=\"\"\"{{\n",
    "        \"(character #1)\": (character #1) is (other character's name)'s (relation). He is (personality traits). (description of appearance).\n",
    "        \"(character #2)\": (character #2) is (other character's name)'s (relation). She is (personality traits). (description of appearance).\n",
    "    }}\"\"\"\n",
    ")\n",
    "\n",
    "user_message_example_2 = generate_message(\n",
    "    role=\"user\",\n",
    "    content=\"\"\"I will provide you with a set of instructions, followed by a sequence of summaries.\n",
    "    The summaries are labeled 'Summary #1' and 'Summary #2'.\n",
    "\n",
    "    Instructions:\n",
    "    Briefly summarize the character (character name) based on Summary #1 and Summary #2.\n",
    "\n",
    "    Summary #1: (contents of summary #1, which contains a description of the character's relations, personality traits and appearance)\n",
    "\n",
    "    Summary #2: (contents of summary #2, which contains a description of the character's relations, personality traits and appearance)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "assistant_message_example_2 = generate_message(\n",
    "    role=\"assistant\",\n",
    "    content=\"(character) is (other character's name)'s (relation). He or she is (personality traits). (description of appearance).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = pd.read_csv(f'data/characters/{book_names[book_index]}_characters.csv')['name'].tolist()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_and_template = system_message, user_message_example_1, assistant_message_example_1, generate_prompt(\n",
    "    summarize_character_from_excerpt_template.format(\n",
    "        parsed_character_json_format=\"\",\n",
    "        parsed_character_names=\"\",\n",
    "        text=\"\"\n",
    "    ))\n",
    "\n",
    "ENCODING_NAME = \"cl100k_base\"\n",
    "CHARACTERS_TO_BE_SUMMARIZED_PER_CHUNK = 5\n",
    "\n",
    "MESSAGES_AND_TEMPLATE_TOKEN_SIZE = num_tokens_from_string(json.dumps(messages_and_template), ENCODING_NAME)\n",
    "MAX_OUTPUT_TOKEN_SIZE = 600 * CHARACTERS_TO_BE_SUMMARIZED_PER_CHUNK\n",
    "MAX_TOKEN_SIZE = 8192\n",
    "MAX_CHUNK_TOKEN_SIZE = MAX_TOKEN_SIZE - MAX_OUTPUT_TOKEN_SIZE - MESSAGES_AND_TEMPLATE_TOKEN_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_character_from_excerpt(character_names, text):\n",
    "\n",
    "    parsed_character_names = parse_character_names(character_names)\n",
    "    parsed_character_json_format = parse_character_json_format(character_names)\n",
    "\n",
    "    tokens_parsed_character_names = num_tokens_from_string(parsed_character_names, ENCODING_NAME)\n",
    "    tokens_character_json_format = num_tokens_from_string(parsed_character_json_format, ENCODING_NAME)\n",
    "\n",
    "    messages = [\n",
    "        system_message,\n",
    "        user_message_example_1,\n",
    "        assistant_message_example_1,\n",
    "        generate_prompt(summarize_character_from_excerpt_template.format(\n",
    "            parsed_character_json_format=parsed_character_json_format,\n",
    "            parsed_character_names=parsed_character_names,\n",
    "            text=text\n",
    "        ))\n",
    "    ]\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = \"https://prod-bb-openai-sweden.openai.azure.com/\", \n",
    "        api_key=os.getenv(\"API_KEY\"),  \n",
    "        api_version=\"2024-02-15-preview\"\n",
    "    )\n",
    "\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages = messages,\n",
    "        temperature=0.15,\n",
    "        max_tokens=MAX_OUTPUT_TOKEN_SIZE - tokens_parsed_character_names - tokens_character_json_format,\n",
    "        top_p=0.85,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "def summarize_characters(character_names, chunks):\n",
    "    summaries_per_chunk = []\n",
    "\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        characters_to_be_summarized = []\n",
    "        for character_name in character_names:\n",
    "            if character_name in chunk['content']:\n",
    "                characters_to_be_summarized.append(character_name)\n",
    "        try:\n",
    "            completion = summarize_character_from_excerpt(characters_to_be_summarized, chunk['content'])\n",
    "            \n",
    "            summary_json = json.loads(completion.choices[0].message.content)\n",
    "            for character_name, summary in summary_json.items():\n",
    "                summaries_per_chunk.append({\"end_sid\": chunk['end_sid'], \"character_name\": character_name, \"summary\": summary})\n",
    "\n",
    "        except Exception as e:    \n",
    "            error_message = f\"An error occurred while summarizing chunk {chunk_index}: {e}\"\n",
    "            with open(\"error_log.txt\", \"a\") as file:\n",
    "                file.write(error_message + \"\\n\")\n",
    "            continue       \n",
    "    summaries_per_chunk_df = pd.DataFrame(summaries_per_chunk)\n",
    "    summaries_per_chunk_path = f\"data/summaries/{book_title}/summaries_per_chunk.csv\"\n",
    "    \n",
    "    if os.path.isfile(summaries_per_chunk_path):\n",
    "        os.remove(summaries_per_chunk_path)\n",
    "\n",
    "    summaries_per_chunk_df.to_csv(summaries_per_chunk_path)\n",
    "\n",
    "def summarize_character_from_summaries(character_name, summary_1, summary_2):\n",
    "    \n",
    "    tokens_character_name = num_tokens_from_string(character_name, ENCODING_NAME)\n",
    "\n",
    "    messages = [\n",
    "        system_message,\n",
    "        user_message_example_2,\n",
    "        assistant_message_example_2,\n",
    "        generate_prompt(summarize_character_from_summaries_template.format(\n",
    "            character_name=character_name, \n",
    "            summaries=summary_1 + \"\\n\" + summary_2\n",
    "        ))\n",
    "    ]\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = \"https://prod-bb-openai-sweden.openai.azure.com/\", \n",
    "        api_key=os.getenv(\"API_KEY\"),  \n",
    "        api_version=\"2024-02-15-preview\"\n",
    "    )\n",
    "\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages = messages,\n",
    "        temperature=0.1,\n",
    "        max_tokens=MAX_OUTPUT_TOKEN_SIZE - tokens_character_name,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "def hierarchically_merge(summaries_per_chunk_df, character_names):\n",
    "    merged_summaries = []\n",
    "    for character_name in character_names:\n",
    "        character_summaries = summaries_per_chunk_df.loc[summaries_per_chunk_df['character_name'] == character_name].to_dict(\"records\")\n",
    "        merged_summaries.append(character_summaries[0])\n",
    "        if len(character_summaries) > 1:\n",
    "            for summary_index in range(len(character_summaries) - 1):\n",
    "                summary_1 = merged_summaries[summary_index]['summary']\n",
    "                summary_2 = character_summaries[summary_index + 1]['summary']\n",
    "                completion = summarize_character_from_summaries(character_name, summary_1, summary_2)\n",
    "                merged_summaries.append({\"end_sid\": character_summaries[summary_index + 1]['end_sid'], \"character_name\": character_name, \"summary\": completion.choices[0].message.content})\n",
    "\n",
    "    merged_summaries_df = pd.DataFrame(merged_summaries)\n",
    "    merged_summaries_path = f\"data/summaries/{book_title}/merged_summaries.csv\"\n",
    "    merged_summaries_exist = os.path.isfile(merged_summaries_path)\n",
    "    merged_summaries_df.to_csv(merged_summaries_path, mode=\"a\", header=not merged_summaries_exist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "character_names = pd.read_csv('data/characters/' + book_names[book_index] + '_characters.csv')['name'].tolist()[CHARACTERS_TO_BE_SUMMARIZED_PER_CHUNK+5:CHARACTERS_TO_BE_SUMMARIZED_PER_CHUNK+10]\n",
    "sentences_df = pd.read_csv('data/sentences/' + book_names[book_index] + '_sentences.csv')\n",
    "\n",
    "sentences_per_chunk = calculate_sentences_per_chunk(sentences_df, ENCODING_NAME, MAX_CHUNK_TOKEN_SIZE)\n",
    "chunks = generate_chunks(sentences_df, sentences_per_chunk, MAX_CHUNK_TOKEN_SIZE, ENCODING_NAME)\n",
    "\n",
    "summarize_characters(character_names, chunks)\n",
    "summaries_per_chunk_df = pd.read_csv(f\"data/summaries/{book_title}/summaries_per_chunk.csv\")\n",
    "summaries_per_chunk_df = summaries_per_chunk_df.sort_values(['character_name','end_sid'])\n",
    "\n",
    "hierarchically_merge(summaries_per_chunk_df, character_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{\"name\": \"Daniel\", \"surname\": \"Dahlberg\"}, {\"name\": \"Axel\", \"surname\": \"Lokrantz\"}]).to_csv(\"test.csv\", mode=\"a\", header=not os.path.isfile(\"test.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
